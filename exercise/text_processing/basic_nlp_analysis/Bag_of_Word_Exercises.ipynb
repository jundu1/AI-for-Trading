{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"table_of_content\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Bag-of-Words Exercises\n",
    "\n",
    "In the following, we will convert our corpus of 10-k documents into bag-of-words, and convert them into document vectors using Term-Frequency-Inverse-Document-Frequency (tf-idf) re-weighting. Afterward, we will compute sentiments and similarity metrics. Since we will be reusing our notebook, so the various sections are linked below:\n",
    "\n",
    "1. <a href=\"#bag_of_word\">Compute bag-of-words </a>: implement `bag_of_words` that converts tokenized words into a dictionary of word-counts\n",
    "\n",
    "2. <a href=\"#sentiment\">Sentiments </a>: using wordlists, compute positive and negative sentiments from bag-of-words. Implement `get_sentiment`\n",
    "\n",
    "For solutions, see [bagofwords_solutions.py](./bagofwords_solutions.py). You can load the functions by simply calling \n",
    "\n",
    "`from bagofwords_solutions import *`\n",
    "\n",
    "# Part 2: Document-Vector Exercises\n",
    "\n",
    "3. <a href=\"#compute_idf\">Compute idf </a>: computing the inverse document frequency, implement `get_idf`\n",
    "\n",
    "4. <a href=\"#compute_tf\">Compute tf </a>: computing the term frequency, implement `get_tf`\n",
    "\n",
    "5. <a href=\"#doc_vector\">Document vector </a>: using the functions `get_idf` and `get_tf`, compute a word_vector for each document in the corpus\n",
    "6. <a href=\"#similarity\">Similarities </a>: comparing two vectors, and compute cosine and jacard similarity metrics. Implement `get_cos` and `get_jac`\n",
    "\n",
    "For solutions, see [bagofwords_solutions.py](./bagofwords_solutions.py). You can load the functions by simply calling \n",
    "\n",
    "`from bagofwords_solutions import *`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialization\n",
    "\n",
    "First we read in our 10-k documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 10-Ks item 1A for CIK = 0001065088 ...\n",
      "skipping EBAY_10k_2017.txt\n",
      "skipping EBAY_10k_2016.txt\n",
      "skipping EBAY_10k_2015.txt\n",
      "skipping EBAY_10k_2014.txt\n",
      "skipping EBAY_10k_2013.txt\n",
      "downloading 10-Ks item 1A for CIK = 0000320193 ...\n",
      "skipping AAPL_10k_2017.txt\n",
      "skipping AAPL_10k_2016.txt\n",
      "skipping AAPL_10k_2015.txt\n",
      "skipping AAPL_10k_2014.txt\n",
      "skipping AAPL_10k_2013.txt\n",
      "downloading 10-Ks item 1A for CIK = 0001310067 ...\n",
      "skipping SHLDQ_10k_2017.txt\n",
      "skipping SHLDQ_10k_2016.txt\n",
      "skipping SHLDQ_10k_2015.txt\n",
      "skipping SHLDQ_10k_2014.txt\n",
      "skipping SHLDQ_10k_2013.txt\n",
      "10-k files:  ['AAPL_10k_2013.txt', 'AAPL_10k_2014.txt', 'AAPL_10k_2015.txt', 'AAPL_10k_2016.txt', 'AAPL_10k_2017.txt', 'EBAY_10k_2013.txt', 'EBAY_10k_2014.txt', 'EBAY_10k_2015.txt', 'EBAY_10k_2016.txt', 'EBAY_10k_2017.txt', 'SHLDQ_10k_2013.txt', 'SHLDQ_10k_2014.txt', 'SHLDQ_10k_2015.txt', 'SHLDQ_10k_2016.txt', 'SHLDQ_10k_2017.txt']\n"
     ]
    }
   ],
   "source": [
    "# download some excerpts from 10-K files\n",
    "from download10k import get_files\n",
    "\n",
    "CIK = {'ebay': '0001065088', 'apple':'0000320193', 'sears': '0001310067'}\n",
    "get_files(CIK['ebay'], 'EBAY')\n",
    "get_files(CIK['apple'], 'AAPL')\n",
    "get_files(CIK['sears'], 'SHLDQ')\n",
    "\n",
    "\n",
    "# get a list of all 10-ks in our directory\n",
    "files=! ls *10k*.txt\n",
    "print(\"10-k files: \",files)\n",
    "files = [open(f,\"r\").read() for f in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we define useful functions to tokenize the texts into words, remove stop-words, and lemmatize and stem our words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# for nice number printing\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# tokenize and clean the text\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# tokenize anything that is not a number and not a symbol\n",
    "word_tokenizer = RegexpTokenizer(r'[^\\d\\W]+')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "sno = SnowballStemmer('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# get our list of stop_words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "# add some extra stopwords\n",
    "stop_words |= {\"may\", \"business\", \"company\", \"could\", \"service\", \"result\", \"product\", \n",
    "               \"operation\", \"include\", \"law\", \"tax\", \"change\", \"financial\", \"require\",\n",
    "               \"cost\", \"market\", \"also\", \"user\", \"plan\", \"actual\", \"cash\", \"other\",\n",
    "               \"thereto\", \"thereof\", \"therefore\"}\n",
    "\n",
    "# useful function to print a dictionary sorted by value (largest first by default)\n",
    "def print_sorted(d, ascending=False, print_top = 15):\n",
    "    factor = 1 if ascending else -1\n",
    "    sorted_list = sorted(d.items(), key=lambda v: factor*v[1])\n",
    "    printed_count = 0\n",
    "    for i, v in sorted_list:\n",
    "        print(\"{}: {:.3f}\".format(i, v))\n",
    "        printed_count += 1\n",
    "        if printed_count >= print_top:\n",
    "            break\n",
    "\n",
    "# convert text into bag-of-words\n",
    "def clean_text(txt):\n",
    "    lemm_txt = [ wnl.lemmatize(wnl.lemmatize(w.lower(),'n'),'v') \\\n",
    "                for w in word_tokenizer.tokenize(txt) if \\\n",
    "                w.isalpha() and w not in stop_words ]\n",
    "    return [ sno.stem(w) for w in lemm_txt if w not in stop_words and len(w) > 2 ]\n",
    "\n",
    "corpus = [clean_text(f) for f in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bag_of_words\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that converts a list of tokenized words into bag-of-words, i.e. a dictionary that outputs the frequency count of a word\n",
    "\n",
    "** python already provide the `collections.Counter` class to perform this, but I encourage you to implement your own function as an exercise\n",
    "\n",
    "<a href=\"#table_of_content\">back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def bag_of_words(words):\n",
    "    count_dict = defaultdict(int)\n",
    "    for word in words:\n",
    "        count_dict[word] +=1\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sentiment\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sentiments\n",
    "Count the fraction of words that appear in a wordlist, returning a sentiment score between 0 and 1:\n",
    "\n",
    "$$\n",
    "\\textrm{score} = \\frac{\\textrm{Number of words in document matching wordlist}}{\\textrm{Number of words in document}}\n",
    "$$\n",
    "\n",
    "Implement the score in a function `get_sentiment(words, wordlist)`, where words is a list of words. Feel free to use the previous `bag_of_words` function. \n",
    "(*for extra challenge, try to code the function in one-line*)\n",
    "\n",
    "Here, I've included a positive and negative wordlist that I constructed by hand. Due to copyright issues, we are not able to provide other commonly used wordlists. I encourage you to try out different wordlists on your own.\n",
    "\n",
    "<a href=\"#table_of_content\">back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wordlist first\n",
    "import pickle\n",
    "\n",
    "with open('positive_words.pickle', 'rb') as f:\n",
    "    positive_words = pickle.load(f)\n",
    "    # also need to stem and lemmatize the text\n",
    "    positive_words = set(clean_text(\" \".join(positive_words)))\n",
    "    \n",
    "with open('negative_words.pickle', 'rb') as f:\n",
    "    negative_words = pickle.load(f)\n",
    "    negative_words = set(clean_text(\" \".join(negative_words)))\n",
    "    \n",
    "# check out the list\n",
    "# print(\"positive words: \", positive_words)\n",
    "# print(\"negative words: \", negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(txt, wordlist):\n",
    "    matching_words = [w for w in txt if w in wordlist]\n",
    "    return len(matching_words) / len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.034  0.033  0.032  0.032  0.031  0.034  0.033  0.034  0.031  0.032\n",
      "  0.037  0.033  0.037  0.039  0.04 ]\n",
      "[ 0.054  0.053  0.052  0.052  0.053  0.051  0.05   0.054  0.061  0.061\n",
      "  0.06   0.059  0.058  0.054  0.059]\n"
     ]
    }
   ],
   "source": [
    "# test your function\n",
    "positive_sentiments = np.array([ get_sentiment(c, positive_words) for c in corpus ])\n",
    "print(positive_sentiments)\n",
    "\n",
    "negative_sentiments = np.array([ get_sentiment(c, negative_words) for c in corpus ])\n",
    "print(negative_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**before continuing part 2, go through the lesson material first!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"compute_idf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Document Vector Exercises\n",
    "\n",
    "## 3. Compute idf\n",
    "Given a corpus, or a list of bag-of-words, we want to compute for each word $w$, the inverse-document-frequency, or ${\\rm idf}(w)$. This can be done in a few steps:\n",
    "\n",
    "1. Gather a set of all the words in all the bag-of-words (python set comes in handy, and the union operator `|` is useful here)\n",
    "\n",
    "\n",
    "2. Loop over each word $w$, and compute ${\\rm df}_w$, the number of documents where this word appears at least once. A dictionary is useful for keeping track of ${\\rm df}_w$\n",
    "\n",
    "\n",
    "3. After computing ${\\rm df}_w$, we can compute ${\\rm idf}(w)$. There are usually two possibilities, the simplest one is \n",
    "$${\\rm idf}(w)=\\frac{N}{{\\rm df}_w}$$\n",
    "Where $N$ is the total number of documents in the corpus and $df_w$ the number of documents that contain the word $w$. Frequently, a logarithm term is added as well\n",
    "$${\\rm idf}(w)=\\log\\frac{N}{{\\rm df}_w}$$\n",
    "One issue with using the logarithm is that when ${\\rm df}_w = N$, ${\\rm idf}(w)=0$, indicating that words common to all documents would be ignored. If we don't want this behavior, we can define ${\\rm idf}(w)=\\log\\left(1+N/{\\rm df}_w\\right)$ or ${\\rm idf}(w)=1+\\log\\left(N/{\\rm df}_w\\right)$ instead. For us, we'll not use the extra +1 for ${\\rm idf}$.\n",
    "\n",
    "In the following, define a function called `get_idf(corpus, include_log=True)` that computes ${\\rm idf}(w)$ for all the words in a corpus, where `corpus` for us is a processed list of bag-of-words (stemmed and lemmatized). The optional parameter `include_log` includes the logarithm in the computation.\n",
    "\n",
    "<a href=\"#table_of_content\">back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute idf\n",
    "import itertools\n",
    "def get_idf(corpus, include_log=True):\n",
    "    all_words = set(itertools.chain.from_iterable(corpus))\n",
    "    df = defaultdict(int)\n",
    "    for word in all_words:\n",
    "        for doc in corpus:\n",
    "            if word in doc:\n",
    "                df[word] +=1\n",
    "    idf = defaultdict(float)\n",
    "    for key, value in df.items():\n",
    "        idf[key] = np.log(len(corpus) / value) if include_log else len(corpus) / value\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should expect to see many idf values = 0! This is by design, because we have ${\\rm idf}(w)=\\log N_d/{\\rm df}_w$ and $N_d/{\\rm df}_w=1$ for the most common words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand: 2.708\n",
      "supplementari: 2.708\n",
      "dedic: 2.708\n",
      "sdk: 2.708\n",
      "cid: 2.708\n",
      "arkansa: 2.708\n",
      "bidder: 2.708\n",
      "shanghai: 2.708\n",
      "popular: 2.708\n",
      "dual: 2.708\n",
      "deadlin: 2.708\n",
      "packag: 2.708\n",
      "beacon: 2.708\n",
      "affirm: 2.708\n",
      "donat: 2.708\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "idf=get_idf(corpus)\n",
    "print_sorted(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"compute_tf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute tf\n",
    "\n",
    "Below we will compute ${\\rm tf}(w,d)$, or the term frequency for a given word $w$, in a given document $d$. Since our ultimate goal is to compute a document vector, we'd like to keep a few things in mind\n",
    "\n",
    "1. Store the ${\\rm tf}(w, d)$ for each word in a document as a dictionary\n",
    "2. Even when a word doesn't appear in the document $d$, we still want to keep a $0$ entry in the dictionary. This is important when we convert the dictionary to a vector, where zero entries are important\n",
    "\n",
    "\n",
    "There are multiple definitions for ${\\rm tf}(w,d)$, the simplest one is\n",
    "\n",
    "$$\n",
    "{\\rm tf}(w,d)=\\frac{f_{w,d}}{a_d}\n",
    "$$\n",
    "\n",
    "where $f_{w,d}$ is the number of occurence of the word $w$ in the document $d$, and $a$ the average occurence of all the words in that document for normalization. Just like ${\\rm idf}(w)$, a logarithm can be added\n",
    "\n",
    "$$\n",
    "{\\rm tf}(w,d)=\\begin{cases}\n",
    "\\frac{1+\\log f_{w,d}}{1+\\log a_d} & f_{w,d} > 0 \\\\\n",
    "0 & f_{w,d} = 0 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Implement the function `get_df(txt, include_log=True)` that computes ${\\rm tf}(w,d)$ for each word in the document (returns a defaultdict(int), so that when supplying words not in the document the dictionary will yield zero instead of an error). Also include the optional parameter `include_log` that enables the additional logarithm term in the computation. I suggest also adding a function called `_tf` that computes the function above. \n",
    "\n",
    "<a href=\"#table_of_content\">back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import *\n",
    "\n",
    "def _tf(freq, avg, include_log=True):\n",
    "    if include_log:\n",
    "        return 0 if freq == 0 else (1 + np.log(freq)) / (1 + np.log(avg))\n",
    "    else:\n",
    "        return freq / avg\n",
    "\n",
    "\n",
    "def get_tf(txt, include_log=True):\n",
    "    freq = bag_of_words(txt)\n",
    "    avg = np.mean(list(freq.values()))\n",
    "    tf = {w:_tf(f,avg, include_log) for w,f in freq.items()}\n",
    "    return defaultdict(int, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compon: 1.925\n",
      "signific: 1.877\n",
      "price: 1.877\n",
      "risk: 1.851\n",
      "affect: 1.837\n",
      "develop: 1.837\n",
      "subject: 1.837\n",
      "parti: 1.823\n",
      "third: 1.793\n",
      "advers: 1.777\n",
      "new: 1.777\n",
      "oper: 1.761\n",
      "custom: 1.744\n",
      "foreign: 1.726\n",
      "continu: 1.726\n"
     ]
    }
   ],
   "source": [
    "tfs = [ get_tf(c) for c in corpus]\n",
    "print_sorted(tfs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"doc_vector\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Document Vector\n",
    "Combine the implementation for ${\\rm tf}(w,d)$ and ${\\rm idf}(w)$ to compute a word-vector for each document in a corpus. Don't forget the zero-padding that is needed when a word appears in some document but not others. \n",
    "\n",
    "Implmenet the function `get_vectors(tf,idf)`, the input is the output computed by `get_tf` and `get_idf`\n",
    "\n",
    "(*optional challenge: implement in one line!*)\n",
    "\n",
    "<a href=\"#table_of_content\">back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(tf, idf):\n",
    "    return np.array([ tf[w]*idf[w] for w in idf ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.235  0.     0.    ...,  0.408  0.     0.809]\n",
      "[ 0.229  0.     0.    ...,  0.397  0.     0.788]\n",
      "[ 0.418  0.     0.    ...,  0.429  0.     0.356]\n",
      "[ 0.276  0.     0.    ...,  0.     0.     0.398]\n",
      "[ 0.273  0.     0.    ...,  0.     0.     0.394]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.337  0.     0.    ...,  0.     0.     0.   ]\n",
      "[ 0.664  0.     0.    ...,  0.548  0.     0.   ]\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "doc_vectors = [ get_vector(tf, idf) for tf in tfs ]\n",
    "\n",
    "for v in doc_vectors:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id =\"similarity\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Similarity\n",
    "\n",
    "Given two word-vectors $\\vec u$ (or $u_i$) and $\\vec v$ (or $v_i$), corresponding to two documents, we want to compute different similarity metrics. \n",
    "\n",
    "1. Cosine similarity, defined by \n",
    "$$\n",
    "{\\rm Sim}_{\\cos} = \\frac{\\vec u \\cdot \\vec v}{|\\vec u| |\\vec v|}\n",
    "$$\n",
    "\n",
    "2. Jaccard similarity, defined by\n",
    "$$\n",
    "{\\rm Sim}_{\\rm Jaccard} = \\frac{\\sum_i \\min\\{u_i, v_i\\}}{\\sum_i \\max\\{u_i, v_i\\}}\n",
    "$$\n",
    "\n",
    "Implement the function similarity as `sim_cis(u,v)` and `sim_jac(u,v)`. Utilize the numpy functions `numpy.linalg.norm` to compute norm of a vector and `np.dot` for computing dot-products. `np.minimum` and `np.maximum` are also useful vectorized pair-wise minimum and maximum functions.\n",
    "\n",
    "(*optional challenge: implement both functions in one line!*)\n",
    "\n",
    "<a href=\"#table_of_content\">back to top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def sim_cos(u,v):\n",
    "    return np.dot(u,v)/(norm(u)*norm(v))\n",
    "\n",
    "def sim_jac(u,v):\n",
    "    return np.sum(np.minimum(u,v))/np.sum(np.maximum(u,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity:\n",
      "[[ 1.     0.95   0.912  0.903  0.89   0.122  0.12   0.136  0.135  0.139\n",
      "   0.017  0.025  0.03   0.041  0.074]\n",
      " [ 0.95   1.     0.961  0.947  0.932  0.116  0.114  0.132  0.127  0.13\n",
      "   0.017  0.024  0.03   0.041  0.071]\n",
      " [ 0.912  0.961  1.     0.986  0.931  0.122  0.121  0.139  0.133  0.137\n",
      "   0.017  0.024  0.032  0.041  0.074]\n",
      " [ 0.903  0.947  0.986  1.     0.941  0.124  0.123  0.138  0.135  0.141\n",
      "   0.018  0.025  0.033  0.043  0.076]\n",
      " [ 0.89   0.932  0.931  0.941  1.     0.121  0.12   0.136  0.134  0.137\n",
      "   0.021  0.027  0.035  0.044  0.073]\n",
      " [ 0.122  0.116  0.122  0.124  0.121  1.     0.893  0.591  0.469  0.473\n",
      "   0.061  0.078  0.115  0.126  0.145]\n",
      " [ 0.12   0.114  0.121  0.123  0.12   0.893  1.     0.601  0.452  0.455\n",
      "   0.059  0.072  0.107  0.119  0.142]\n",
      " [ 0.136  0.132  0.139  0.138  0.136  0.591  0.601  1.     0.654  0.661\n",
      "   0.069  0.083  0.116  0.126  0.147]\n",
      " [ 0.135  0.127  0.133  0.135  0.134  0.469  0.452  0.654  1.     0.954\n",
      "   0.071  0.084  0.095  0.101  0.133]\n",
      " [ 0.139  0.13   0.137  0.141  0.137  0.473  0.455  0.661  0.954  1.     0.074\n",
      "   0.087  0.098  0.102  0.135]\n",
      " [ 0.017  0.017  0.017  0.018  0.021  0.061  0.059  0.069  0.071  0.074  1.\n",
      "   0.843  0.558  0.435  0.354]\n",
      " [ 0.025  0.024  0.024  0.025  0.027  0.078  0.072  0.083  0.084  0.087\n",
      "   0.843  1.     0.602  0.482  0.387]\n",
      " [ 0.03   0.03   0.032  0.033  0.035  0.115  0.107  0.116  0.095  0.098\n",
      "   0.558  0.602  1.     0.719  0.507]\n",
      " [ 0.041  0.041  0.041  0.043  0.044  0.126  0.119  0.126  0.101  0.102\n",
      "   0.435  0.482  0.719  1.     0.672]\n",
      " [ 0.074  0.071  0.074  0.076  0.073  0.145  0.142  0.147  0.133  0.135\n",
      "   0.354  0.387  0.507  0.672  1.   ]]\n",
      "\n",
      "Jaccard Similarity:\n",
      "[[ 1.     0.901  0.836  0.825  0.803  0.069  0.066  0.094  0.11   0.111\n",
      "   0.018  0.028  0.045  0.055  0.075]\n",
      " [ 0.901  1.     0.918  0.9    0.88   0.069  0.066  0.095  0.108  0.11\n",
      "   0.019  0.029  0.046  0.055  0.075]\n",
      " [ 0.836  0.918  1.     0.977  0.892  0.073  0.07   0.1    0.113  0.114\n",
      "   0.02   0.029  0.048  0.056  0.078]\n",
      " [ 0.825  0.9    0.977  1.     0.904  0.073  0.07   0.099  0.114  0.116\n",
      "   0.02   0.03   0.049  0.057  0.079]\n",
      " [ 0.803  0.88   0.892  0.904  1.     0.072  0.069  0.099  0.113  0.114\n",
      "   0.022  0.032  0.05   0.059  0.079]\n",
      " [ 0.069  0.069  0.073  0.073  0.072  1.     0.795  0.419  0.287  0.287\n",
      "   0.026  0.035  0.062  0.077  0.1  ]\n",
      " [ 0.066  0.066  0.07   0.07   0.069  0.795  1.     0.418  0.271  0.271\n",
      "   0.024  0.033  0.056  0.071  0.095]\n",
      " [ 0.094  0.095  0.1    0.099  0.099  0.419  0.418  1.     0.497  0.499\n",
      "   0.035  0.046  0.075  0.091  0.116]\n",
      " [ 0.11   0.108  0.113  0.114  0.113  0.287  0.271  0.497  1.     0.927\n",
      "   0.04   0.052  0.076  0.088  0.114]\n",
      " [ 0.111  0.11   0.114  0.116  0.114  0.287  0.271  0.499  0.927  1.     0.042\n",
      "   0.054  0.078  0.089  0.116]\n",
      " [ 0.018  0.019  0.02   0.02   0.022  0.026  0.024  0.035  0.04   0.042  1.\n",
      "   0.701  0.367  0.266  0.189]\n",
      " [ 0.028  0.029  0.029  0.03   0.032  0.035  0.033  0.046  0.052  0.054\n",
      "   0.701  1.     0.46   0.342  0.24 ]\n",
      " [ 0.045  0.046  0.048  0.049  0.05   0.062  0.056  0.075  0.076  0.078\n",
      "   0.367  0.46   1.     0.621  0.397]\n",
      " [ 0.055  0.055  0.056  0.057  0.059  0.077  0.071  0.091  0.088  0.089\n",
      "   0.266  0.342  0.621  1.     0.54 ]\n",
      " [ 0.075  0.075  0.078  0.079  0.079  0.1    0.095  0.116  0.114  0.116\n",
      "   0.189  0.24   0.397  0.54   1.   ]]\n"
     ]
    }
   ],
   "source": [
    "# test your co\n",
    "# compute all the pairwise similarity metrics\n",
    "size = len(doc_vectors)\n",
    "matrix_cos = np.zeros((size,size))\n",
    "matrix_jac = np.zeros((size,size))\n",
    "\n",
    "for i in range(size):\n",
    "    for j in range(size):\n",
    "        u = doc_vectors[i]\n",
    "        v = doc_vectors[j]\n",
    "        matrix_cos[i][j] = sim_cos(u,v)\n",
    "        matrix_jac[i][j] = sim_jac(u,v)\n",
    "        \n",
    "print(\"Cosine Similarity:\")\n",
    "print(matrix_cos)\n",
    "\n",
    "print()\n",
    "print(\"Jaccard Similarity:\")\n",
    "print(matrix_jac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Job! You've finished all the exercises!\n",
    "\n",
    "Here is an optional bonus challenge. We often need to debug other people's code to figure out what's wrong. It's particularly difficult when the code doesn't give errors but computes the wrong quantity. Can you describe why the following implementations for some of the exercises above may be wrong? Highlight the words at the bottom to reveal the solutions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf_wrong(corpus, include_log=True):\n",
    "    freq = defaultdict(int)\n",
    "    for c in corpus:\n",
    "        for w in c:\n",
    "            freq[w] += 1\n",
    "        \n",
    "    N = len(corpus)\n",
    "    if include_log:\n",
    "        return { w:log(N/freq[w]) for w in freq }\n",
    "    else:\n",
    "        return { w:N/freq[w] for w in freq }\n",
    "\n",
    "\n",
    "def get_sentiment_wrong(txt, wordlist):\n",
    "    matching_words = [ w for w in wordlist if w in txt ]\n",
    "    return len(matching_words)/len(txt)\n",
    "\n",
    "def get_vectors_wrong(tf, idf):\n",
    "    return np.array([ tf[w]*idf[w] for w in tf ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions\n",
    "\n",
    "Drag your mouse over the white space below this cell, and you'll see details about the solutions.  Or, if it's easier, just double-click on the white space below this cell, which will reveal the cell with hidden text.  Also, please check out the file [bagofwords_solutions.py](bagofwords_solutions.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution\n",
    "\n",
    "get_idf: the defaultdict freq here computes the total number of occurences in all the document. We only want to count it once when a word appears in a document. This may lead to a document frequency larger than N, leading to a negative idf! \n",
    "\n",
    "get_sentiment_wrong: if a word w appears twice in the document, it won't be counted properly!\n",
    "\n",
    "get_vectors_wrong: tf may not contain all the words in idf. We need zero padding for words that appear in idf but not in tf! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
